<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>[cs231n] 11강 리뷰 | elefunt-devlog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="[cs231n] 11강 리뷰" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Segmentation &amp;&amp; Detection" />
<meta property="og:description" content="Segmentation &amp;&amp; Detection" />
<link rel="canonical" href="https://elephantoid.github.io/elefunt-devlog/cs231n/2021/07/18/cs231n_11.html" />
<meta property="og:url" content="https://elephantoid.github.io/elefunt-devlog/cs231n/2021/07/18/cs231n_11.html" />
<meta property="og:site_name" content="elefunt-devlog" />
<meta property="og:image" content="https://elephantoid.github.io/elefunt-devlog/images/cs231n/cs231n-11.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-07-18T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"Segmentation &amp;&amp; Detection","url":"https://elephantoid.github.io/elefunt-devlog/cs231n/2021/07/18/cs231n_11.html","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://elephantoid.github.io/elefunt-devlog/cs231n/2021/07/18/cs231n_11.html"},"headline":"[cs231n] 11강 리뷰","dateModified":"2021-07-18T00:00:00-05:00","datePublished":"2021-07-18T00:00:00-05:00","image":"https://elephantoid.github.io/elefunt-devlog/images/cs231n/cs231n-11.png","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/elefunt-devlog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://elephantoid.github.io/elefunt-devlog/feed.xml" title="elefunt-devlog" /><link rel="shortcut icon" type="image/x-icon" href="/elefunt-devlog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/elefunt-devlog/">elefunt-devlog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/elefunt-devlog/about/">About Me</a><a class="page-link" href="/elefunt-devlog/gitInfo/">gitInfo</a><a class="page-link" href="/elefunt-devlog/sample/">sample</a><a class="page-link" href="/elefunt-devlog/search/">Search</a><a class="page-link" href="/elefunt-devlog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">[cs231n] 11강 리뷰</h1><p class="page-description">Segmentation && Detection</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-07-18T00:00:00-05:00" itemprop="datePublished">
        Jul 18, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      7 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/elefunt-devlog/categories/#cs231n">cs231n</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p><a href="https://www.youtube.com/watch?v=nDPWywWRIRo&amp;t=1s"><img src="http://img.youtube.com/vi/nDPWywWRIRo/0.jpg" alt="cs231n 11강 유튜브" /></a></p>

<blockquote>
  <p>Tip: 이미지를 클릭하시면 바로 유튜브 강의를 볼 수 있습니다.</p>
</blockquote>

<p><img src="/elefunt-devlog/images/cs231n/cs231n-11/c1.PNG" alt="" /></p>

<p>앞서 Classification과 다양한 모델들 그리고 아키텍처들을 살펴봤습니다. 이번 강의에서는 Computer Vision분야의 다른 task들을 알아볼 것입니다.</p>

<h1 id="today-topic">Today topic</h1>

<ul>
  <li>Semantic Segmentation</li>
  <li>Instance Segmentation</li>
  <li>Classification + Localization</li>
  <li>Object Detection</li>
</ul>

<h1 id="semantic-segmentation">Semantic Segmentation</h1>

<p><img src="/elefunt-devlog/images/cs231n/cs231n-11/c2.PNG" alt="" /></p>

<p>Semantic Segmentation은 이미지의 픽셀들이 어떤 클래스에 속하는지 예측하는 과정이다. 예를들어 위 사진 처럼 각각의 픽셀이 고양이인지 잔디인지 하늘인지 예측하여 맞춰야한다.<br />
하지만 문제점이 하나 있다면 개별 객체는 구분을 못한다..(우측 소 사진 참고)</p>

<h2 id="idea-1-sliding-window">IDEA 1: Sliding Window</h2>

<p><img src="/elefunt-devlog/images/cs231n/cs231n-11/c3.PNG" alt="" /></p>

<ul>
  <li>Patch를 만들고, 사진의 전체를 모두 돌면서 한 픽셀이 어느 클래스에 들어가는지 예측하는 방법</li>
  <li>이 방법은 computation이 비쌉니다. 딱 봐도 계산량이 어마무시할 것으로 예상됩니다</li>
  <li>또한 겹쳐지는 Patch의 공유된 feature들을 재사용하지 않는 문제점이 있습니다.</li>
  <li>추후 instance segmentation에서 이 문제를 해결할 예정입니다.</li>
</ul>

<h2 id="idea-2-fully-convolutional-networks">IDEA 2: Fully Convolutional Networks</h2>

<p><img src="/elefunt-devlog/images/cs231n/cs231n-11/c4.PNG" alt="" /></p>

<ul>
  <li>3x3 filter를 사용해 이미지 크기를 유지하며 convolution에 넣음</li>
  <li>한번에 모든 픽셀을 예측할 수 있도록 설계</li>
  <li>원본 이미지 크기를 유지하면서 진행하기 때문에 연산량이 매우 높다는 문제점</li>
</ul>

<h3 id="fully-convolutional-with-downsampling-and-upsampling">Fully Convolutional with Downsampling and Upsampling</h3>

<p>그 동안의 문제점을 보완하기 위해 나온 구조입니다.</p>

<p><img src="/elefunt-devlog/images/cs231n/cs231n-11/c5.PNG" alt="" /></p>

<p>Downsampling 후에 다시 Upsampling을 해서 다시 원본처럼 만들어주는 방법인데 문제가 하나 있습니다.<br />
다시 이미지의 사이즈를 늘릴 때 빈공간을 어떻게 채울것인가..?</p>

<font size="5" color="blue">Solution 1: Upsampling</font>

<p><img src="/elefunt-devlog/images/cs231n/cs231n-11/c5.PNG" alt="" /></p>

<p><strong>Nearest Neighbor</strong>
매우 1차원적인 방법으로 빈공간을 원래 있던 값으로 모두 채우는 방법이다.</p>

<p><strong>Bad of Nails</strong>
unpooling region에만 값을 채우고 나머지는 0으로 채움</p>

<font size="5" color="blue">Solution 2: Max unpooling</font>

<p><img src="/elefunt-devlog/images/cs231n/cs231n-11/c6.PNG" alt="" /></p>

<p>네트워크는 대칭적인 경향이 있는데 각 pooling과 unpooling을 연괏짓는 방법입니다.<br />
Max pooling시에 선정된 Region을 기억하고 있다가 unpooling할 때 해당 Region에 값을 채우고 나머지는 0으로 통일합니다.<br />
하지만 특정 Feature Map의 공간성을 잃는다는 단점이 있다</p>

<font size="5" color="blue">Solution 2: Transpose Convolution</font>

<p><img src="/elefunt-devlog/images/cs231n/cs231n-11/c7.PNG" alt="" /></p>

<p>위 그림은 일반적인 3x3 Conv입니다. stride=2 pad=1<br />
filter는 2 pixels 움직여 output에 값을 채워주게 됩니다.<br />
이 때 중요한 점은 입력값과 출력값 사이에 위치 정보가 연결되어 있습니다.
Transpose convolutional은 이 방식을 거꾸로 한다고 생각하시면 편합니다.</p>

<p><img src="/elefunt-devlog/images/cs231n/cs231n-11/c8.PNG" alt="" /></p>

<p>똑같이 필터를 움직이며 input의 모든 값을 채워 넣습니다.<br />
output에서는 transpose convolution간에 Receptive Field가 겹칠 수 있습니다.
여기서 특이하게 겹치는 부분은 덧셈을 하게되는데 수식이 그렇게 되어 있으며 Receptive Field의 크기에 따라 magnitudes가 달라지는 문제가 될 수 있습니다.<br />
아직 이부분은 연구가 진행중이라고 합니다.</p>

<p>행렬곱으로 보면 조금 더 이해가 잘 된다고 하네요..<br />
1D로 된 convolutiond으로 예시를 들어보겠습니다.</p>

<p><img src="/elefunt-devlog/images/cs231n/cs231n-11/c9.PNG" alt="" /></p>

<ul>
  <li>
    <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>필터</mtext><mi>x</mi><mo>=</mo><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo separator="true">,</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">필터x=(x,y,z)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord hangul_fallback">필</span><span class="mord hangul_fallback">터</span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="mclose">)</span></span></span></span></span>
  </li>
  <li>
    <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>입력</mtext><mi>a</mi><mo>=</mo><mo stretchy="false">(</mo><mi>a</mi><mo separator="true">,</mo><mi>b</mi><mo separator="true">,</mo><mi>c</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">입력a=(a,b,c,d)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord hangul_fallback">입</span><span class="mord hangul_fallback">력</span><span class="mord mathdefault">a</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">b</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">c</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">d</span><span class="mclose">)</span></span></span></span></span>
  </li>
  <li>Stride= 1</li>
  <li>Padding =1</li>
</ul>

<p>Xa가 만들어지는 것 까지는 오케이!<br />
Ta도 오케이! normal과 비슷하게 나옵니다.<br />
하지만 stride가 2일 경우에는?</p>

<p><img src="/elefunt-devlog/images/cs231n/cs231n-11/c10.PNG" alt="" /></p>

<p>stride가 1보다 큰 경우에는 더이상 convolution이 아니라고 설명합니다.<br />
stride 2 T convolution은 이제 normal과 근본적으로 다른 연산이 되기 때문입니다.
output이 input에 비해 크게 확장된 것을 알 수 있습니다.</p>

<p>이제 downsampling과 upsampling 후 모든 픽셀에 대한 cross-entropy를 계산하면 전체 네트워크를 end-to-end로 학습시킬 수 있습니다.</p>

<h1 id="classification--localization">Classification + Localization</h1>

<p>이미지가 어떤 카테고리에 속하는지 뿐만 아니라 실제 객체에 Bounding Box(네모)를 그려서 이미지에서 어디에 있는지 알 수 있습니다.<br />
Classification + Localization은 object detection과는 구별됩니다.<br />
localization에서는 <strong>오직 하나의 객체</strong>만 찾아 레이블을 매기고 위치를 찾습니다.</p>

<p><img src="/elefunt-devlog/images/cs231n/cs231n-11/c11.PNG" alt="" /></p>

<ul>
  <li>input: 이미지</li>
  <li>output1: Class Scores(Softmax loss)</li>
  <li>output2: Bbox 좌표(예측한 Bbox와 실제 Bbox의 차이 측정 loss like L2)</li>
</ul>

<p>학습시에 loss가 두 개가 존재합니다. (지도 학습 가정)<br />
학습 이미지에는 카테고리 레이블과 해당 객체의 Bbox를 동시에 가지고 있어야 합니다.</p>

<details>
<summary>Q: 왜 두 loss를 동시에 학습시키나요? 오분류에 대해 Bbox가 있으면 어떻게 되나요?</summary>
<div>

    <p>A: 일반적으로 괜찮습니다. 오분류의 문제에서 까다로울 수 있음. Bbox를 카테고리마다 하나씩 예측해서 Ground Truth 카테고리에 속한 예측된 Bbox에만 Loss를 연결시킵니다. 실제로 아주 유용합니다.</p>

  </div>
</details>

<details>
<summary>Q: 두 개의 loss 단위가 다른데 Gradient 계산시 문제가 되지 않나요?</summary>
<div>

    <p>A: multi-task loss입니다. 네트워크 가중치들의 각각의 미분 값을 계산해야 합니다. 그래서 두 개의 미분값이 생기고 둘 다 최소화 시켜야합니다. 사실 이 loss의 가중치를 조절하는 하이퍼 파라미터가 있습니다. 두 loss의 가중치 합이 최종 loss입니다. loss의 가중치 합에 대한 gradient를 계산하는 것입니다.<br />
이 하이퍼 파라미터를 튜닝하는 것은 어려운데 그 이유는 이전에 봐왔던 것과는 다르게 이 하이퍼 파라미터는 loss값 자체를 변경하게 됩니다. 일반적인 방법은loss값을 비교하는 것이 아니라 모델의 성능지표를 보는 것입니다.</p>

  </div>
</details>

<details>
<summary>Q: 앞쪽 큰 네트워크만 고정시키고 각 FC-layer만 학습시키는건 어떤가요?</summary>
<div>

    <p>A: 괜찮은 방법입니다. transfer learning의 관점에서 보면 fine tune을 하는것이 성능 향상에 더 도움됩니다. 실제로 사람들이 많이 하는 방법은 네트워크를 Freeze하고(동결시켜서 W가 학습되지 않음) 두 FC-layer를 학습시킵니다. 그 후 FC-layer가 수렴하면 전체 네트워크를 Fine tune!</p>

  </div>
</details>

<h1 id="object-detection">Object Detection</h1>

<p><img src="/elefunt-devlog/images/cs231n/cs231n-11/c12.PNG" alt="" /></p>

<p>방금 본 Classification + Localization과 비슷하게 보이지만 큰 차이가 있습니다.<br />
<strong>Object Detection은 입력 이미지의 객체 수를 알지 못합니다.</strong> input image에 따라 다른 output이 나옵니다. 따라서 Regression로 풀기 상당히 어렵습니다.</p>

<h2 id="idea-1-sliding-window-1">IDEA 1: Sliding Window</h2>

<p><img src="/elefunt-devlog/images/cs231n/cs231n-11/c13.PNG" alt="" /></p>

<p>Sementic Segmentation에서 봤던 것과 유사합니다.<br />
crop(잘라낸)한 이미지를 CNN에 넣고 classification을 돌립니다 객체가 있는지 없는지 그 객채가 Dog인지 Cat인지 확인합니다.<br />
그 후 Background 카테고리를 추가해줍니다. 나중에 Bbox를 그릴 때 Background에 속한 것들은 제외됩니다.<br />
게다가 input image에 Objects가 몇 개가 존재할지 어디에 있을지 모릅니다.<br />
이러한 문제 때문에 Sliding Window를 할려면 너무나도 많은 경우의 수가 존재합니다.</p>

<h2 id="idea-2-region-proposals">IDEA 2: Region Proposals</h2>

<p><img src="/elefunt-devlog/images/cs231n/cs231n-11/c14.PNG" alt="" /></p>

<p>사실 이 방법은 Deep learning을 사용하지는 않습니다.
이미지 내에 뭉쳐진 곳들을 찾아 Object가 있을법한 1000개의 box를 제공합니다.<br />
이 지역들은 객체가 있을지도 모르는 후보 영역들입니다.</p>

<h3 id="selective-search">Selective Search</h3>

<p>2000개의 Region Proposals을 만들어 냅니다. 비교적 빠릅니다. 노이즈가 심하고 실제 객체가 아닐수도 있지만 꽤 괜찮습니다.</p>

<p>이제는 모든 위치를 다 고려하는 것이 아니라 Region Proposals Networks를 적용해서 객체가 있을법한 Region Proposals를 얻어냅니다. 이것을 CNN의 입력으로 넣으면 이전 방법보다 훨씬 계산량을 감소시킬 수 있을 것입니다.</p>

<p>R-CNN에서 나온 내용입니다.</p>

<h2 id="r-cnn">R-CNN</h2>

<p><img src="/elefunt-devlog/images/cs231n/cs231n-11/c15.PNG" alt="" /></p>

<ol>
  <li>Region Proposals Network 수행</li>
  <li>Selectiv Search 수행 2000개 ROI(Region of Interesting=Region Proposals) 획득</li>
  <li>ROI 사이즈 동일하게 resize</li>
  <li>각각 ROI CNN 수행</li>
  <li>최종 Classification SVMs 수행</li>
</ol>

<p><strong><font size="5">문제점 </font></strong></p>

<ul>
  <li>계산 비용이 많이 든다.</li>
  <li>용량(Memory)도 많이 든다.</li>
  <li>학습 과정도 느리다. (논문 기준 81시간)</li>
  <li>Test Time도 느리다. (한 이미지당 30초)</li>
  <li>학습이 되지 않는 Region Proposal이 존재</li>
  <li><strong>Backprop가 안된다.</strong></li>
</ul>

<h2 id="fast-r-cnn">Fast R-CNN</h2>

<p>앞서 R-CNN은 다양한 문제점이 있었습니다. 해당되는 문제점들 중에서 속도 측면에서 더 빨라진 모델입니다.</p>

<p><img src="/elefunt-devlog/images/cs231n/cs231n-11/c16.PNG" alt="" /></p>

<p>기존 모델은 이미지에서 바로 Selective Serch를 통해서 각각의 Region proposals를 CNN에 넣고 작업 시간을 오래 잡아먹었습니다.<br />
하지만 Fast R-CNN 모델은 먼저 이미지에서 ROI 영역 추출 후 CNN을 거쳐서 Feature Map을 뽑고 거기에 처음에 뽑아둔 ROI영역을 적용 시키기 때문에 Feature들을 공유하면서 더 빠르게 작업을 수행할 수 있습니다. 즉, input이 R-CNN은 2000장 정도였다면 현재는 1장입니다.</p>

<p>ROI Pooling
이 부분은 Fast R-CNN의 핵심이라고 할 수 있습니다. ROI를 FC layer에 넣기 위해서는 같은 크기여야 하는데 Selective Search를 통해 구해진 ROI 영역은 제각각 다릅니다. 따라서 이 Resolution의 크기를 맞춰주기 위해 ROI pooling을 수행합니다.<br />
간단하게 FC layer에 넣기 위해 제각각인 모양을 통일시켜준다고 생각하시면 됩니다.<br />
그 방법은 크기가 다른 Feature Map의 Region마다 Stride를 다르게 Max pooling하는 것입니다.</p>

<p><img src="https://user-images.githubusercontent.com/70086728/127449634-1e31b99c-063b-4f30-8e1e-0d6454600c63.gif" alt="roi_pooling" /></p>

<p>마지막 부분은 R-CNN과 비슷하지만 softmax를 사용해서 classification을 진행 했습니다.</p>

<p><img src="/elefunt-devlog/images/cs231n/cs231n-11/c18.png" alt="" />
R-CNN이나 SPP-Net에 비해 뛰어난 성능을 보였고 기존 R-CNN의 문제도 많이 해결했지만 Region Proposal에 걸리는 2.3초 중 2초의 시간 때문에 병목현상이 생기는 문제가 있지만 그것을 해결하기 위해 아래 모델이 나왔습니다.</p>

<h2 id="faster-r-cnn">Faster R-CNN</h2>
<p><img src="/elefunt-devlog/images/cs231n/cs231n-11/c17.PNG" alt="" /></p>

<p>다른 부분들은 다 비슷하지만 여기에 RPN이라는 개념이 나옵니다. 해당 보라색 부분과 Selective Search를 제외하면 동일한 구조입니다.
RPN의 input은 Feature map이고 output은 Object proposals의 Sample입니다.</p>

<p>원래 네트워크 밖에서 Selective Search를 하며 2초가 걸렸던 것을 이제 네트워크 안에서 RPN이 해결합니다.</p>

<p>총 4개의 Loss function이 필요합니다.</p>
<ol>
  <li>RPN classify object / not object (binary classificaion) ROI에 객체가 있는지 없는지</li>
  <li>RPN regress box coordinate 예측한 BBox에 관한 것</li>
  <li>Final classification score(object classes)</li>
  <li>Final box coordinates 앞서 만든 regression을 보정해줍니다</li>
</ol>

<p>RPN은 Region Proposal Network을 의미한다.
Faster R-CNN에서는 총 9개의 anchor box를 사용하는데</p>

<p>Anchor box란 물체의 모양이 어떻게 생겼는지 모르니 서로 다른 비율의 Bounding Box를 만들고, 모두 시도를 해봐서 최적의 Bounding Box를 찾기 위해 사용하는 것이라 생각하면 이해하기 쉽다.</p>

<p>물체가 있을 확률을 IoU(Intersection over Union)을 통해 계산한다.</p>

<p>내가 예측한 Boundnig Box의 크기와 실제 Groundtruth의 값을 비교해보는 것이다.</p>

<details>
<summary>Q: 어떻게 RPN GT가 없는데 어떻게 RPN을 학습시키는가?</summary>
<div>

    <p>A:<br />
GT(Ground truth) object와 일정 threshold이상 겹치는 proposals이 있을 것입니다. 이런 regoin proposal은 positive라고 예측해야 합니다. 반면 조금만 겹치면 Negative라고 예측해야합니다.</p>

  </div>
</details>

<p>Faster R-CNN에서 흥미로운 점은 Region proposals 자체도 학습되었기 때문에 RPN과 새로운 데이터 사이의 간극이 존재할 수도 있을 것입니다. 이 경우에는 새로운 데이터에 맞게 region proplosal을 새롭게 학습시키면 그만입니다.</p>

<h2 id="yolossd">Yolo/SSD</h2>

<p><img src="/elefunt-devlog/images/cs231n/cs231n-11/c19.PNG" alt="" /></p>

<p>그림 우측의 B는 base BBox의 offset(4개) + Confidence score(1개)로 구성<br />
C는 C개의 카테고리에 대한 Classification Score</p>

<p>지금까지는 2-stage Detector였습니다 classificaion과 localization을 따로 했지만 Yolo와 SSD는 한 번의 forward pass로 처리합니다.</p>

<p>주요 아이디어는 각 Task를 따로 계산하지않고 하나의 regression문제로 풀어보는 것입니다.</p>

<p>거대한 CNN을 통해서 예측을 할텐데 그 결과값 중 하나는 Bbox의 offset을 예측할 수 있을 것입니다. 실제 위치가 되려면 base BBox를 얼만큼 옮겨야 하는지를 뜻합니다. 그리고 각 BBOx에 대해서 Classification Score를 구합니다. 이 카테고리에 속한 객체가 존재할 가능성을 의미합니다.</p>

<font size="4">You Only Look one</font>
<p>Yolo같은 경우는 이미지의 Grid를 나눌 때 7*7로 고정을 한 다음, Bounding Box를 찾습니다</p>

<font size="4">Single Short Detection</font>
<p>후보 base BBox와 GT Objects를 매칭시키는 방법<br />
SSD의 같은 경우 Grid를 나눌 때, 여러 feature map을 참고하여 세분화하여 Grid를 나누고 Bounding Box를 찾는다.</p>

<h1 id="instance-segmentation">Instance Segmentation</h1>
<p>위에서 배운 것들의 총 집합입니다. 각각의 객체를 찾아서 BBox를 그리는게 아니라 객체 별 Segmentation Mask를 예측해야합니다. 각 객체에 해당하는 픽셀을 예측해야 하는 문제입니다.</p>

<h2 id="mask-r-cnn">Mask R-CNN</h2>
<p><img src="/elefunt-devlog/images/cs231n/cs231n-11/c20.PNG" alt="" /></p>

<p>Faster R-CNN와 유사합니다. ROI Align(Pooling)까지는 동일하지만 거기서 BBox마다 Segmentation mask를 예측하도록 합니다.<br />
RPN으로 뽑은 ROI영역 내에서 각각 sementic segmentation을 수행합니다.<br />
위쪽으로는 각 Region proposal이 어떤 카테고리에 속하는지 계산합니다. 또한 Region proposal의 좌표를 보정해주는 BBox Regression도 예측합니다.
아래쪽은 각 픽셀마다 객체인지 아닌지 확인합니다.</p>

<h1 id="reference">Reference</h1>

<p><a href="https://naokishibuya.medium.com/up-sampling-with-transposed-convolution-9ae4f2df52d0">Transpose Convolution</a><br />
<a href="https://taeyoung96.github.io/cs231n/CS231n_11/">cs231n<em>11</em>리뷰</a><br />
<a href="https://nuggy875.tistory.com/21">R-CNN</a><br />
<a href="https://nuggy875.tistory.com/21">Fast&amp;Faster_R-CNN</a></p>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="elephantoid/elefunt-devlog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/elefunt-devlog/cs231n/2021/07/18/cs231n_11.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/elefunt-devlog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/elefunt-devlog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/elefunt-devlog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" title="fastai"><svg class="svg-icon grey"><use xlink:href="/elefunt-devlog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/elefunt-devlog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
