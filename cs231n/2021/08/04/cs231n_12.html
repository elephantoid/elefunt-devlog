<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>[cs231n] 12강 리뷰 | elefunt-devlog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="[cs231n] 12강 리뷰" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Visualization and Understanding" />
<meta property="og:description" content="Visualization and Understanding" />
<link rel="canonical" href="https://elephantoid.github.io/elefunt-devlog/cs231n/2021/08/04/cs231n_12.html" />
<meta property="og:url" content="https://elephantoid.github.io/elefunt-devlog/cs231n/2021/08/04/cs231n_12.html" />
<meta property="og:site_name" content="elefunt-devlog" />
<meta property="og:image" content="https://elephantoid.github.io/elefunt-devlog/images/cs231n/cs231n-12.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-08-04T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"Visualization and Understanding","url":"https://elephantoid.github.io/elefunt-devlog/cs231n/2021/08/04/cs231n_12.html","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://elephantoid.github.io/elefunt-devlog/cs231n/2021/08/04/cs231n_12.html"},"headline":"[cs231n] 12강 리뷰","dateModified":"2021-08-04T00:00:00-05:00","datePublished":"2021-08-04T00:00:00-05:00","image":"https://elephantoid.github.io/elefunt-devlog/images/cs231n/cs231n-12.png","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/elefunt-devlog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://elephantoid.github.io/elefunt-devlog/feed.xml" title="elefunt-devlog" /><link rel="shortcut icon" type="image/x-icon" href="/elefunt-devlog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/elefunt-devlog/">elefunt-devlog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/elefunt-devlog/about/">About Me</a><a class="page-link" href="/elefunt-devlog/gitInfo/">gitInfo</a><a class="page-link" href="/elefunt-devlog/search/">Search</a><a class="page-link" href="/elefunt-devlog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">[cs231n] 12강 리뷰</h1><p class="page-description">Visualization and Understanding</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-08-04T00:00:00-05:00" itemprop="datePublished">
        Aug 4, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      8 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/elefunt-devlog/categories/#cs231n">cs231n</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>12강 Visualizing and Understanding</p>

<p><img src="https://user-images.githubusercontent.com/70086728/128139166-2153aead-2d17-4d68-b417-c73eb5bdd01d.PNG" alt="31" /></p>

<ul>
  <li>Activation values를 기반으로 해당 특징이 무엇을 찾고 있는지</li>
  <li>Gradient를 이용해 새로운 이미지를 만드는 방법</li>
  <li>위와 같은 방법들로 CNN을 좀 더 깊숙이 이해하고 활용할 수 있도록 해보는 것이 목표입니다.</li>
</ul>

<h2 id="how-to-understand-inside-of-cnn">How to Understand inside of CNN</h2>

<p><img src="https://user-images.githubusercontent.com/70086728/127942611-cdacdc1e-bb88-4ec2-aa2a-fe5f3b9590f6.PNG" alt="1" title="PyTorch Model Zoo - pretrained model" />
<br />
가장 쉽게 찾아볼 수 있는 것은 첫 번째 Layer입니다.<br />
AlexNet같은 경우는 3x11x11 형태의 필터를 64개 갖고 있습니다. 이 필터들은 sliding window로 이미지를 쓱쓱 돌 것입니다. 그렇게 돌고 나서 가중치와 내적한 결과가 첫 번째 Conv Layer의 출력입니다.</p>

<p>하지만 첫 번째 Layer는 이미지와 직접적으로 연산을 수행하기 때문에 해당 필터가 이미지에서 어떤 것을 찾고 있는지 알기 쉽지 않습니다.
그래서 11x11x3 rgb를 시각화하여 본 것이 위 그림입니다. 가장 많이 찾을 수 있는 것은 엣지 성분입니다. (흰/검, 보색)</p>

<p>신기한 점은 CNN을 어떤 모델/데이터로 학습하건 첫 번째 레이어는 전부 다 비슷하게 생겼습니다. 첫 강의에서 나온 인간의 시각체계 또한 oriented edge를 찾는 것을 생각해본다면 매우 신기합니다.</p>

<p>&lt;/div&gt;</p>
<details>
<summary>Q: 필터의 가중치를 시각화해서 필터가 무엇을 찾고 있는지 어떻게 알아요?! </summary>
<div>
    <p>이는 Templete Matchin이나 내적을 생각하면 된다고합니다. Templete Vector와 임의의 데이터를 내적에서 어떤 Scaler output을 계산하는데 필터값을 가장 활성화시키는 입력을 생각해보면 입력 값과 필터의 가중치 값이 동일한 경우입니다.<br />
  <br />
  또한, 내적의 경우에는 내적 값을 최대화시키는 방법은 동일한 값 두 개를 내적하는 것입니다.<br />
  <br />
  그러므로 이미지와 필터의 내적을 구한 첫 번째 레이어의 가중치를 시각화 시키면 첫 번째 레이어가 무엇을 찾는지 알 수 있는 것입니다.</p>
  </div>
</details>

<p><br />
<br /></p>

<p><img src="https://user-images.githubusercontent.com/70086728/127943723-838604cc-52e3-4f7d-a449-10e0b55fbce4.PNG" alt="2" />
다음은 중간 과정에서의 시각화입니다.</p>

<ul>
  <li>Layer 1: 7x7 16개</li>
  <li>Layer 2: (input 16 channel 7x7) 20개</li>
  <li>Layer 3: (input 20 channel 7x7) 20개
Layer 2에서 시각화한 내용은 Layer 2의 결과를 최대화시키는(활성화시키는) 첫 번째 레이어의 출력 패턴이 무엇인지 입니다.</li>
</ul>

<p><br />
<br /></p>

<p><img src="https://user-images.githubusercontent.com/70086728/127945438-894f9831-a940-46ba-89fc-2c1d7ff7e686.PNG" alt="3" />
마지막 레이어의 시각화입니다.
CNN의 마지막 레이어에는 1000개의 클래스 스코어가 있습니다. 바로 직전에는 4096-dim 특징 벡터를 갖고 있습니다.</p>

<ol>
  <li>많은 이미지를 CNN 돌림</li>
  <li>각 이미지에서 나온 4096-dim 특징 벡터 저장</li>
  <li>Nearest Neighbors를 이용하여 시각화</li>
</ol>

<p><br />
<br /></p>

<h1 id="activations">Activations</h1>

<font size="4" color="blue">Nearest Neighbors</font>

<p>맨 왼쪽의 사진은 <code class="language-plaintext highlighter-rouge">이미지 픽셀 공간</code>에서의 Nearest Neighbors입니다.
픽셀 공간에서는 비슷한 형태나 색을 갖고 있다면 가깝다고 생각하는 것을 확인
<br />
<code class="language-plaintext highlighter-rouge">4096-dim 특징 벡터</code>를 이용해 계산한 Nearest Neighbors는 서로 픽셀값이 다르기도 하지만 특징을 중점으로 계산하기 때문에 픽셀 공간보다 잘 분류합니다.</p>

<p><br />
<br />
<br /></p>

<font size="4" color="blue">Dimensionality Reduction</font>
<p><img src="https://user-images.githubusercontent.com/70086728/127946290-14305780-2ff4-4444-ab50-eb0cba42d148.PNG" alt="4" /></p>

<p>차원 축소라는 단어를 생각하면 단연 제일 먼저 떠올리는 것은 PCA(Principle Component Analysis)일 것입니다. 고차원 특징 벡터들을 원하는 dimension으로 압축시키는 기법입니다.<br />
PCA보다 더 강력한 알고리즘인 <code class="language-plaintext highlighter-rouge">t-SNE(t-distributed stochastic Neighbor embeddings))</code>입니다.<br />
위 사진은 t-SNE를 이용해서 4096-dim 특징 벡터를 2-dim으로 압축한 사진입니다. 각각의 군집들이 특징별로 묶어져 위치해 있는 것을 알 수 있습니다.</p>

<p><br />
<br />
<br /></p>

<h2 id="how-to-visualize-features">How to visualize features</h2>

<font size="4" color="blue">Maximally Activating Patches</font>

<p><img src="https://user-images.githubusercontent.com/70086728/127947327-567528d4-7481-4770-b090-7c0104c3955c.PNG" alt="5" />
어떤 이미지가 들어와야 각 뉴런들의 활성이 최대화되는지 시각화하는 방법</p>

<ol>
  <li>채널 중의 하나를 지정(Channel 17)</li>
  <li>conv를 돌릴 때마다 지정한 채널의 값들을 저장</li>
  <li>어떤 이미지의 일부분(Pathes)이 지정된 채널(특징 맵)을 최대로 활성화시키는지 확인</li>
</ol>

<p><br />
<br />
<br /></p>

<font size="4" color="blue">Occlusion Experiments</font>
<p><img src="https://user-images.githubusercontent.com/70086728/127947334-be8215f9-fdaf-477a-a3f0-3eed02f8eec8.PNG" alt="6" />
입력의 어떤 부분이 분류를 결정짓는 근거가 되는지에 관한 실험</p>

<ol>
  <li>mask를 이용해서 이미지를 일부 가리고(가린 부분은 데이터셋의 평균값으로 채움)</li>
  <li>네트워크가 이 이미지를 예측한 확률을 기록</li>
  <li>occluded(가림) patch 를 전체 이미지에 slide
예측확률이 낮을수록 해당 부분이 분류를 결정짓는 중요한 부분인 것을 알 수 있다.</li>
</ol>

<font size="4" color="blue">Saliency Maps</font>
<p><img src="https://user-images.githubusercontent.com/70086728/127947336-074168c4-83bd-4234-9b95-6d17e524a591.PNG" alt="7" />
입력 이미지의 각 픽셀들에 대해 예측한 <code class="language-plaintext highlighter-rouge">클래스 스코어</code>의 그레디언트를 계산하는 방법
어떤 픽셀이 분류하는 데 있어서 어떤 픽셀들이 필요한지 알 수 있다
<br />
<br />
<br /></p>

<font size="4" color="blue">Gudied back propagation</font>
<p><img src="https://user-images.githubusercontent.com/70086728/127970216-d90e4fb7-0abd-42ae-a00d-16bf4de645e7.PNG" alt="8" /></p>

<p>네트워크의 중간 뉴런을 하나 선택<br />
입력 이미지의 어떤 부분이 내가 선택한 뉴런의 값에 영향을 주는지 찾는 것<br />
즉, 입력 이미지의 각 픽셀에 대한 <code class="language-plaintext highlighter-rouge">네트워크 중간 뉴런</code>의 그레디언트를 계산</p>

<p>조금의 트릭을 이용해 더 깨끗한 이미지를 얻을 수 있다.= Gudied backprop</p>

<ul>
  <li>ReLU의 그레디언트 부호가 양수면 그대로 통과</li>
  <li>음수면 backprop를 하지 않음</li>
</ul>

<p>위 두 가지 방법은 <strong>고정된</strong> 입력 이미지에 대한 연산을 수행할 뿐입니다.</p>

<p>해당 뉴런을 활성화시킬 수 있는 어떤 <strong>일반적인</strong> 입력 이미지가 있을까?</p>

<p><br />
<br />
<br /></p>

<font size="4" color="blue">Gradient ascent</font>
<p><img src="https://user-images.githubusercontent.com/70086728/127971459-6b7935f6-182d-447c-9368-4c2419f2605c.PNG" alt="9" />
지금까지는 Loss를 최소화시켜 네트워크를 학습시키기 위해 Gradient decent를 사용했습니다.<br />
하지만 여기선 네트워크의 가중치를 모두 고정시킵니다. 그리고 Gradient ascent를 통해 중간 뉴런 혹은 클래스를 최대화시키는 입력 이미지의 픽셀들을 만들어냅니다.</p>

<p>여기서 Regularization term이 필요한데 기존에는 학습 데이터에 대해 과적합 방지용이었지만 여기서는 생성된 이미지가 특정 네트워크의 특성에 과적합 되는 것을 방지합니다.
특성</p>

<ol>
  <li>이미지가 특정 뉴런의 값을 최대화시키는 방향으로 생성</li>
  <li>이미지가 자연스러워 보여야 함
<br />
<br />
<br /></li>
</ol>

<p><img src="https://user-images.githubusercontent.com/70086728/127971463-b24b561f-980c-4402-b1b9-15739ad0ce83.PNG" alt="10" /></p>

<ol>
  <li>initialize image가 필요(Zeros, uniform, noise 등으로 초기화)</li>
  <li>이미지를 네트워크에 통과, 관심 있는 뉴런 스코어 계산</li>
  <li>이미지 각 픽셀에 대한 해당 뉴런 스코어의 그레디언트를 계산하여 backprop 진행</li>
  <li>Gradient ascent로 업데이트</li>
</ol>

<p><br />
<br /></p>

<p><img src="https://user-images.githubusercontent.com/70086728/127971464-d77aeeb0-0e75-463a-a4ef-01e3f0655aa1.PNG" alt="11" />
여기서 간단히 L2 norm을 사용(딱히 의도는 없음)
스코어를 최대화시키는 이미지 사진을 보면 각각의 다양한 물체가 중첩되어 있는 것을 볼 수 있습니다.</p>

<p><br />
<br /></p>

<p><img src="https://user-images.githubusercontent.com/70086728/127977875-5db9d768-302e-491d-bd73-4f7f7dd0f465.PNG" alt="12" />
조금 더 괜찮은 Regularization으로 이미지를 깔끔하게 만들 수 있습니다.</p>

<ul>
  <li>기본적인 L2</li>
</ul>

<p><strong>최적화 과정 중</strong></p>

<ul>
  <li>주기적으로 가우시안 블러를 적용</li>
  <li>주기적으로 값이 작은 픽셀들을 모두 0으로 만듦</li>
  <li>낮은 기울기의 픽셀값 중 일부는 0으로</li>
</ul>

<p>생성된 이미지를 더 좋은 특성을 가진 이미지 집합으로 주기적으로 매핑시키는 방법</p>

<p>이 방법은 최종 클래스 스코어뿐만 아니라 중간 뉴런에 대해서도 가능합니다.</p>

<p>이미지 생성 문제에서 사전 지식(priors)을 추가하게 된다면 아주 리얼한 이미지를 만들어 낼 수 있습니다. (with feature inversion network)</p>

<p><br />
<br /></p>

<font size="4" color="blue">Fooling Image, Adversarial Examples</font>

<p><img src="https://user-images.githubusercontent.com/70086728/127979041-08deb1ac-aca2-4ace-b172-e530c6bfc688.PNG" alt="13" /></p>

<p>이미지 픽셀의 그레디언트를 이용해 이렇게 이미지를 합성하는 강력한 방법<br />
이미지를 속이는 것!<br />
임의의 이미지를 선택한 후, 다른 이미지의 점수를 최대화합니다.
코끼리의 사진에 코알라의 점수를 최대화하기 위해 코끼리 이미지를 조금씩 코알라로 바꾸다 보면 네트워크는 코끼리 사진을 코알라로 분류합니다</p>

<ul>
  <li>(1) Start from an arbitrary image</li>
  <li>(2) Pick an arbitrary class</li>
  <li>(3) Modify the image to maximize the class</li>
  <li>(4) Repeat until network is fooled</li>
</ul>

<font size="4" color="blue">Deep Dream</font>

<p><img src="https://user-images.githubusercontent.com/70086728/128128598-ec43912d-410e-4bbc-bb91-ece7dcaa6461.PNG" alt="14" /></p>

<p>과학적 가치의 측면에서 보자면 단순히 재미만을 위한 것입니다.
똑같이 입력 이미지를 네트워크에 통과시키고 backprop할 때 해당 레이어의 그레디언트를 Activation값으로 설정합니다.
이는 네트워크에 의해 검출된 이미지의 특징들을 증폭시키려는 것으로 해석할 수 있습니다.
왜냐하면, 해당 레이어에 어떤 특징들이 있던지 그 특징들을 그레디언트로 설정하면 이는 네트워크가 이미지에서 이미 뽑아낸 특징들을 더욱 증폭시키는 역할을 하는 것입니다.
또한, L2 norm을 통해 레이어의 특징을 최대화시킵니다.</p>

<p>1/(x^2)가 있을 때 activation에 대한 그레디언트는 x입니다. 따라서 그레디언트를 보낼 때 activation값 자체를 보내게 되면 1/(x^2)의 그레디언트를 계산하는 것과 동치입니다. 해당 레이어의 특징들(activations)의 norm을 최대화시키는 것과도 동치입니다. 하지만 실제 구현은 이를 명시적으로 계산하지 않고 그레디언트만 뒤로 보내줍니다.
<img src="https://user-images.githubusercontent.com/70086728/128129045-c47be130-8044-4806-908f-91c57bff614a.PNG" alt="15" /></p>

<ol>
  <li>Jitter image
이미지를 조금씩 움직이는 트릭
Regularizer 역할을 해서 부드러운 이미지를 만들어줍니다.</li>
  <li>L1 norm
이미지 합성에서 아주 유용함</li>
  <li>Clipping
이미지가 존재할 수 있는 공간으로 매핑시키는 방법</li>
</ol>

<ul>
  <li>Result</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/70086728/128129097-0460d433-2096-4697-beeb-65510e14dcfe.PNG" alt="16" />
‘개’가 많이 보이는 이유는 이 네트워크가 Image Net의 1000개의 카테고리를 학습한 네트워크인데 그중 200개가 ‘개’라서 많이 보이는 것입니다.</p>

<p><img src="https://user-images.githubusercontent.com/70086728/128129290-bc8e0cb3-7f0d-4fe0-8b4a-5f9cf4f546ab.PNG" alt="17" />
MIT place로 학습시킨 네트워크입니다.
멀티 스케일 프로세싱을 수행해서 작은 이미지로 Deep Dream을 수행하고 점점 이미지 크기를 늘리고 최종 스케일에서 앞의 과정을 반복해서 만듭니다.</p>

<p><a href="https://github.com/google/deepdream">Deep Dream Code</a></p>

<font size="4" color="blue">Feature Inversion</font>

<p><img src="https://user-images.githubusercontent.com/70086728/128130398-863adad4-53af-40aa-8971-f40b1ded0b16.PNG" alt="18" /></p>

<p>이 방법 또한 다양한 레이어에서 이미지의 어떤 요소를 포착하고 있는지 짐작할 수 있게 도와줍니다.</p>

<p>네트워크를 통과하며 나온 특징(activation map)을 저장해둡니다. 그리고 이 특징을 이용해서 이미지를 재구성할 텐데요.<br />
해당 레이어의 특징 벡터로부터 이미지를 재구성해보면, 이미지의 어떤 정보가 특징 벡터에서 포착되는지를 짐작할 수 있을 것입니다.</p>

<p>Point</p>

<ul>
  <li>스코어 최대화 대신 특징 벡터 간의 거리를 최소화(기존 계산해둔 거와 생성한 이미지로 계산한 특징 벡터)</li>
  <li>toal variation Regularization<br />
상하좌우 인접 픽셀 간의 차이에 대한 패널티 부여</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/70086728/128130730-60b902d7-17f6-42dc-80ff-04aa0ff7c8b5.PNG" alt="19" />
처음 이미지가 들어오고서는 재구성했을 때 원본과 거의 차이점이 없지만, 네트워크를 통과하면서 주요 특징은 남지만, 저수준의 정보(디테일,텍스쳐,색, 등)들이 사라지는 것을 볼 수 있습니다</p>

<font size="4" color="blue">Texture Synthesis(텍츠처합성)</font>

<p><img src="https://user-images.githubusercontent.com/70086728/128132831-73251ad8-ac7e-4308-9dd6-a3088db40596.PNG" alt="20" /></p>

<p>문제는 input 패치가 있을 때, 동일한 텍스처인 더 큰 패치를 생성하는 것입니다.<br />
이러한 텍스처 합성은 Computer Graghics에서는 아주 오래된 문제입니다.
Nearest Neighbor도 잘 되며 신경망 대신 scan line을 따라서 한 픽셀씩 이미지를 생성해 나가는 방식입니다.</p>

<ol>
  <li>현재 생성해야 할 픽셀 주변의 이미 생성된 픽셀들을 살펴보기</li>
  <li>입력 패치에서 가장 가까운 픽셀을 계산</li>
  <li>입력 패치로부터 한 픽셀을 복사해 넣는 방식</li>
</ol>

<p>자세히 알 필요는 없어요~<br />
하지만 복잡한 텍스처에서는 단순히 복사하는 것으로는 문제를 풀 수 없을 수도 있습니다.</p>

<font size="4" color="blue">Nerual Texture Synthesis</font>
<p><img src="https://user-images.githubusercontent.com/70086728/128133579-4ee78729-575d-47b9-8ce8-2f0d026f673d.PNG" alt="21" /></p>

<ol>
  <li>입력 이미지 네트워크 통과</li>
  <li>특정 레이어에서 특징 맵(Activation Map) 가져오기(CxHxW)</li>
  <li>특징 맵을 갖고 입력 이미지의 텍스처 기술자(descriptor)를 계산</li>
</ol>

<ul>
  <li>특징 맵에서 서로 다른 두 개의 특징 벡터를 뽑기(빨, 파)</li>
  <li>두 벡터의 외적을 계산 CxC 행렬 만듦
이 CxC행렬은 이미지 내 서로 다른 두 지점에 있는 특징 간의 co-occurrence를 담고 있다</li>
</ul>

<p>서로 다른 공간에서 동시에 활성화되는 특징이 무엇인지 2차 모멘트를 통해 어느 정도 포착 가능합니다.</p>

<p><img src="https://user-images.githubusercontent.com/70086728/128133581-cb4f13d6-e76d-4a5e-93d3-c6b61bea8048.PNG" alt="22" />
위 과정을 HxW 그리드에서 전부 수행하고 결과에 대한 평균을 계산하면 CxC Gram matrix를 얻을 수 있습니다.</p>

<p>이 Gram matrix를 입력 이미지의 텍스처 기술자로 사용합니다.</p>

<ul>
  <li>공간 정보를 모두 날려버림</li>
</ul>

<p>이미지의 각 지점 값들을 평균화해버렸기 때문이다. 대신 co-occurrence를만을 포착하고 있습니다.</p>

<p>물론 공분산으로도 계산할 수 있지만, 계산 비용이 너무 크다는 단점이 있습니다.
<img src="https://user-images.githubusercontent.com/70086728/128133593-62d0778e-6cb7-4e87-9470-d49d33a05450.PNG" alt="23" />
Gradient ascent procedure과 유사한 과정입니다.</p>

<ol>
  <li>pretrained 모델 내려받기(VGG를 선호한다고 해요)</li>
  <li>input image를 CNN을 통과시키며 모든 레이어의 activations를 기록 Layer_i는 \C_i\times H_i\times W_i(\) 의 쉐입</li>
  <li>각각의 레이어에서 Gram matrix를 계산</li>
  <li>생성해야 할 이미지를 랜덤으로 초기화</li>
  <li>다시 CNN에 generated image를 통과시키며 Gram matrix를 계산</li>
  <li>원본 이미지와 생성된 이미지의 Gram matrix 간의 차이를 L2 norm을 이용해 Loss로 계산</li>
  <li>backprop를 통해 생성된 이미지의 픽셀의 그레디언트를 계산</li>
  <li>Gradient ascent를 이용해 이미지의 픽셀을 업데이트</li>
  <li>5단계로 돌아가 버려! (여러 번 반복)</li>
</ol>

<p><img src="https://user-images.githubusercontent.com/70086728/128135370-11ef34e0-3686-42c8-b2b4-14cac6d262ee.PNG" alt="24" /></p>

<font size="4" color="blue">Nerual Style Transfer</font>

<p><img src="https://user-images.githubusercontent.com/70086728/128136407-576f2a82-a4cc-4996-8998-d5163879d7d1.PNG" alt="25" /></p>

<p>앞서 배운 두 가지 Texture Synthesis와 Feature reconstruction을 합친 것입니다.</p>

<p>입력으로 2가지가 들어갑니다.</p>

<ul>
  <li>Content Image : 최종 이미지가 이렇게 생겼으면 좋겠어!</li>
  <li>Style Image: 이런 분위기(스타일)로 만들어줘!</li>
</ul>

<p>Style Transfer는 Content 이미지의 feature reconstruction loss와 style 이미지의 gram matrix loss도 최소화하는 방식으로 최적화하여 생성합니다.</p>

<p><img src="https://user-images.githubusercontent.com/70086728/128136903-68cef6b5-b7c3-4781-a07f-c298b9966f52.PNG" alt="26" /></p>

<p>네트워크에 두 가지 input을 넣고 gram matrix와 feature map을 계산<br />
최종 출력 이미지는 랜덤 노이즈로 초기화<br />
forward/backprop 과정에서 Gradient ascent로 업데이트를 수백 번 반복하면 끝!</p>

<p><a href="https://github.com/jcjohnson/neural-style">Style Transfer Code</a></p>

<font size="4" color="blue">Fast Style Transfer</font>

<p><img src="https://user-images.githubusercontent.com/70086728/128139152-2d16b250-bcea-404a-baf4-593b924138ed.PNG" alt="27" /></p>

<p>4K 이미지를 만드는 건 계산 비용이 너무 많이 들어가고 forward/backward를 수백 번 반복해야 하는 문제점이 있습니다.</p>

<p>즉 너무 느리다는 건데 이 문제점을 해결하는 방안으로 Style Transfer를 위한 또 다른 네트워크를 학습하는 것입니다.</p>

<ol>
  <li>Style 이미지를 고정</li>
  <li>Content 이미지만을 입력으로 받아서 결과를 출력하는 단일 네트워크를 학습</li>
</ol>

<p>학습 시에는 content/style loss를 동시에 학습 네트워크 가중치 업데이트합니다.</p>

<p>학습은 좀 오래 걸리지만 한 번 학습을 시키고 나면 이미지를 네트워크에 통과시키면 결과를 바로 볼 수 있다.</p>

<p><img src="https://user-images.githubusercontent.com/70086728/128139158-af61a938-01dd-41f3-bb3e-3c60dc68d414.PNG" alt="28" /></p>

<font size="4" color="blue">One network, Many Styles</font>

<p><img src="https://user-images.githubusercontent.com/70086728/128139161-49a3bf69-87ec-419f-979f-e7e2b061690b.PNG" alt="29" />
Google에서 나온 논문으로 하나의 네트워크로 다양한 Style을 생성해내는 방법</p>

<p><img src="https://user-images.githubusercontent.com/70086728/128139163-5400d770-578c-4cc4-b96e-39a03f099ecc.PNG" alt="30" />
style blending도 가능 서로 다른 스타일을 학습하면서 섞기</p>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="elephantoid/elefunt-devlog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/elefunt-devlog/cs231n/2021/08/04/cs231n_12.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/elefunt-devlog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/elefunt-devlog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/elefunt-devlog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>This is development blog for ML/DL.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" title="fastai"><svg class="svg-icon grey"><use xlink:href="/elefunt-devlog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/elefunt-devlog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
