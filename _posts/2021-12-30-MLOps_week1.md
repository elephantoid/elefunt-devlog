---
title: "MLOps ê¸°ì´ˆ: 1 ì£¼ì°¨"
description: "W&B"
layout: post
toc: false
comments: true
search_exclude: true
image: "images/MLOps/mlops_1.png"
---

ì´ì œëŠ” ë§ì€ ë¶„ë“¤ì´ ì•Œê³  ê³„ì‹ ,,,

ë˜ ëˆ„êµ¬ì—ê²ŒëŠ” ê½¤ ì¹œìˆ™í•œ,,,

ë„¤. 1 ì£¼ì°¨ëŠ” **`Weights and Bias(**W&B)`ì…ë‹ˆë‹¤

ì‚¬ì‹¤ ì´ë²ˆë…„ë„ ìƒë°˜ê¸°ì— ê°€ì§œ ì—°êµ¬ì†Œì—ì„œ í•´ì»¤í†¤ì„ ì§„í–‰í–ˆì—ˆìŠµë‹ˆë‹¤. ê·¸ ë•Œ ìš°ì—°íˆ ì°¸ê°€í•˜ê²Œ ë˜ì–´ Baselineì„ ì„¤ëª…í•´ì£¼ì‹œë©´ì„œ ì´ ëª¨ë‹ˆí„°ë§ ë„êµ¬ì¸ W&Bë¥¼ ì„¤ëª…í•´ì£¼ì…¨ì—ˆëŠ”ë°ìš”. í•œ ë‘ ë²ˆ ì“°ê³  ì§€ì†ì ìœ¼ë¡œ í™œìš©í•˜ì§€ ëª»í–ˆë˜ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì´ ê¸°íšŒë¥¼ í†µí•´ì„œ ì¢€ ë” ì¹œí•´ì§ˆë ¤ê³  í•©ë‹ˆë‹¤.

[MLOps-Basics ê¹ƒí—™](https://github.com/graviraja/MLOps-Basics)

> Notice: 0ì£¼ì°¨ì˜ transformer ëª¨ë¸ì„ ì˜ ê³µë¶€í•˜ê³  ì˜¤ì…”ì•¼í•©ë‹ˆë‹¤. í•´ë‹¹ ëª¨ë¸ì„ ê°œì„ í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì‹œì‘ë©ë‹ˆë‹¤.

> ì‹¤í–‰í™˜ê²½: Colab
>
> ![image](https://user-images.githubusercontent.com/70086728/147925232-c9daa56d-0d22-4f8d-8519-8be9e442e96b.png)
>
> ![image](https://user-images.githubusercontent.com/70086728/147925253-0aae3d74-b50e-47b3-a5d8-d42e07e4959b.png)

# ML ëª¨ë¸ ëª¨ë‹ˆí„°ë§ ëª¨ë¸ì„ ëª¨ë‹ˆí„°ë§ í•´ì•¼ í•˜ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?

ë§ì€ ì´ìœ ê°€ ìˆìŠµë‹ˆë‹¤. ì˜ˆì¸¡ì˜ ì •í™•ì„±ì„ ì´í•´í•˜ê³  ì˜ˆì¸¡ ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ë©° ëª¨ë¸ì„ ì¡°ì •í•˜ì—¬ ì™„ë²½í•˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì¼ë°˜ì ìœ¼ë¡œ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ê³ , ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•´ ë‹¤ë¥¸ ëª¨ë¸ì„ ì‹œë„í•˜ê³ , ëª¨ë¸ê³¼ ì…ë ¥ ë°ì´í„° ê°„ì˜ ì—°ê²°ì„ í™•ì¸í•˜ê³ , ê³ ê¸‰ í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•˜ì—¬ ì‹¤í—˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.(ì—„ì²­ë‚œ ì‹œê°„ê³¼ ë…¸ë ¥ì´ í•„ìš”í•˜ê² ì£ ..) ê·¸ëŸ°ë° ì´ ëª¨ë“  ê²ƒì„ í•œ ê³³ì— ê¸°ë¡í•˜ë©´ ë” ë¹ ë¥´ê³  ë” ë‚˜ì€ í†µì°°ë ¥ì„ ì–»ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.

ëª¨ë¸ì´ ì›í™œí•˜ê²Œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ê°€ì¥ ì‰¬ìš´ ë°©ë²•ì€ `ML ëª¨ë¸ ëª¨ë‹ˆí„°ë§ ë„êµ¬`ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ë˜í•œ ì „ìš© ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ íŒ€ê³¼ í˜‘ì—…í•˜ê³ , ë‹¤ë¥¸ ì‚¬ëŒë“¤ê³¼ ì‘ì—…ì„ ê³µìœ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŒ€ì´ í˜‘ì—…í•˜ê³ , ëª¨ë¸ ìƒì„± ë° ì¶”ê°€ ëª¨ë‹ˆí„°ë§ì— ì°¸ì—¬í•  ìˆ˜ ìˆëŠ” ê³µìœ  ê³µê°„ì…ë‹ˆë‹¤. ëª¨ë¸ì—ì„œ ë°œìƒí•˜ëŠ” ìƒí™©ì— ëŒ€í•œ ì‹¤ì‹œê°„ í†µì°°ë ¥ì´ ìˆìœ¼ë©´ ì•„ì´ë””ì–´, ìƒê° ë° ê´€ì°°ì„ êµí™˜í•˜ê³  ì˜¤ë¥˜ë¥¼ ì°¾ê¸°ê°€ ë” ì‰½ìŠµë‹ˆë‹¤. ê¸°ê³„ í•™ìŠµì„ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë§ì´ ìˆìŠµë‹ˆë‹¤.

- Comet
- MLFlow
- Neptune
- TensorBoard
- Weights and Bias
- and many more

ì´ ê²Œì‹œë¬¼ì—ì„œëŠ” ë‹¤ìŒ ì£¼ì œë¥¼ ë‹¤ë£° ê²ƒì…ë‹ˆë‹¤.

1. `W&Bë¡œ ê¸°ë³¸ ë¡œê¹…ì„ êµ¬ì„±í•˜ëŠ” ë°©ë²•`
2. `ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•˜ê³  W&Bì— ê¸°ë¡í•˜ëŠ” ë°©ë²•`
3. `W&Bì— í”Œë¡¯ì„ ì¶”ê°€í•˜ëŠ” ë°©ë²•`
4. `W&Bì— ë°ì´í„° ìƒ˜í”Œì„ ì¶”ê°€í•˜ëŠ” ë°©ë²•`

> ì°¸ê³ : ê¸°ê³„ í•™ìŠµ, Pytorch Lightningì— ëŒ€í•œ ê¸°ë³¸ ì§€ì‹ì´ í•„ìš”í•©ë‹ˆë‹¤.

# **Weights and Bias Configuration**

W&Bë¥¼ ì´ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ê³„ì •ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤. (ê³µê³µ í”„ë¡œì íŠ¸ ë° 100GB ìŠ¤í† ë¦¬ì§€ì˜ ê²½ìš° ë¬´ë£Œ). ê³„ì •ì´ ìƒì„±ë˜ë©´ ë¡œê·¸ì¸í•´ì•¼ í•©ë‹ˆë‹¤

ğŸ”**[https://wandb.ai/authorize](https://wandb.ai/authorize)**

![image](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5f871f00-0777-476b-b6cc-883d32ec6bb1/Untitled.png)

ê°„í¸í•˜ê²Œ Githubì„ í†µí•´ ë¡œê·¸ì¸í–ˆìŠµë‹ˆë‹¤.

![image](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/bb31a805-e91c-4399-835a-63e4d7a738fd/Untitled.png)

ë¡œê·¸ì¸ í•˜ê²Œ ë˜ë©´ ë°”ë¡œ ì¸ì¦ì„ ìœ„í•œ í‚¤ë¥¼ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![image](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/a355c704-93f4-4c2a-b6a8-1a74e6cc7dc9/Untitled.png)

**ì¸ì¦ì„ ìœ„í•œ í‚¤ë¥¼ ì—¬ê¸°ì— ë„£ìœ¼ì‹œë©´ ë©ë‹ˆë‹¤!**

ë¡œê·¸ì¸ì´ ì˜ ë˜ì—ˆëŠ”ì§€ ê¶ê¸ˆí•˜ì‹œë‹¤ë©´ ë‹¤ì‹œ ì‹¤í–‰í•´ ë³´ì„¸ìš”. ì˜ ë˜ì—ˆë‹¤ë©´ ì•„ë˜ì™€ ê°™ì´ ë‚˜ì˜µë‹ˆë‹¤.

`wandb: Currently logged in as: elefun (use `wandb login --relogin` to force relogin)`

# ğŸ“Œ**Configuring**

W&Bì—ì„œ í”„ë¡œì íŠ¸ë¥¼ ìƒì„±í•˜ê³  ì—¬ê¸°ì— ê°™ì€ ì´ë¦„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
ëª¨ë“  ì‹¤í—˜ì´ í•´ë‹¹ í”„ë¡œì íŠ¸ì— ë¡œê·¸ì¸ ë˜ì–´ ê¸°ë¡ ë˜ë„ë¡ í•©ë‹ˆë‹¤.

```python
from pytorch_lightning.loggers import WandbLogger
wandb_logger = WandbLogger(project="MLOps Basics")
```

ì´ì œ `logger`ë¥¼ `Trainer`ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.

```python
trainer = pl.Trainer(
        max_epochs=3,
        logger=wandb_logger,
        callbacks=[checkpoint_callback],
    )
```

ì´ì œ ëª¨ë“  ë¡œê·¸ëŠ” W&Bì— ê¸°ë¡ë  ê²ƒì…ë‹ˆë‹¤.

---

![image](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/72d03066-d450-4e05-a91f-a8783b6396cf/Untitled.png)

PyCharmì„ ì´ìš©í•˜ì‹œëŠ” ë¶„ë“¤ì€ ì´ë ‡ê²Œ í•˜ì‹œë©´ ë©ë‹ˆë‹¤.

1. í„°ë¯¸ë„ì—ì„œ `wandb init` ìœ¼ë¡œ í•œ ë²ˆ ì´ˆê¸°í™” ì‹œí‚¤ê¸°
2. `(3) Create New` ì˜µì…˜ìœ¼ë¡œ ë ˆí¬ ë§Œë“¤ê¸°
3. train.pyì— `wandb.init(project="mlops-basics")` ì¶”ê°€í•˜ê¸°

# ğŸ“**Metrics**

Metrics ê³„ì‚°ì€ ë•Œë•Œë¡œ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤í–‰íˆ pytorch lighting íŒ€ì€ ëª¨ë“  ì£¼ìš” Metricsì„ í¬í•¨í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ `torchmetrics`ì„ êµ¬ì¶•í•˜ê³  ìˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [ì„¤ëª…ì„œ](https://torchmetrics.readthedocs.io/en/latest/)ë¥¼ í™•ì¸í•˜ì‹­ì‹œì˜¤.

ë¬¸ì œëŠ” ë¶„ë¥˜ì— ê´€í•œ ê²ƒì´ë¯€ë¡œ ë‹¤ìŒê³¼ ê°™ì€ Metricsì„ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

`Accuracy`, `Precision`, `Recall`, `F1`

ì¼ë‹¨ importë¥¼ í•´ì•¼ê² ì£ 

```python
import torchmetrics
```

`__init__` ì— metricsì„ ì„ ì–¸í•´ì¤ë‹ˆë‹¤

```python
class ColaModel(pl.LightningModule):
    def __init__(self, model_name="google/bert_uncased_L-2_H-128_A-2", lr=3e-5):
        self.train_accuracy_metric = torchmetrics.Accuracy()
        self.val_accuracy_metric = torchmetrics.Accuracy()
        self.f1_metric = torchmetrics.F1(num_classes=self.num_classes)
        self.precision_macro_metric = torchmetrics.Precision(
            average="macro", num_classes=self.num_classes
        )
        self.recall_macro_metric = torchmetrics.Recall(
            average="macro", num_classes=self.num_classes
        )
        self.precision_micro_metric = torchmetrics.Precision(average="micro")
        self.recall_micro_metric = torchmetrics.Recall(average="micro")
```

ìœ„ì™€ ê°™ì´ í•´ë‘ë©´ í•™ìŠµí•  ë•Œ, ê²€ì¦í•  ë•Œ, í…ŒìŠ¤íŠ¸í•  ë•Œ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

Pytorch Lightning Module âš¡ï¸ì—ëŠ” ë©”íŠ¸ë¦­ ê³„ì‚°ì„ êµ¬í˜„í•  ìœ„ì¹˜ë¥¼ ì‰½ê²Œ ì§€ì •í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤.

ì¼ë°˜ì ìœ¼ë¡œ ê³„ì‚°ë˜ëŠ” ë‘ ê°€ì§€ ì£¼ìš” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

- **`training_step`**: training data ë°°ì¹˜ê°€ ì²˜ë¦¬ë˜ëŠ” ê³³ì…ë‹ˆë‹¤. Â **`training loss`**,Â **`training_accuracy`**Â ì´ëŸ° ë©”íŠ¸ë¦­ì´ ì—¬ê¸°ì„œ ê³„ì‚° ë©ë‹ˆë‹¤
- **`validation_step`**: validation data ë°°ì¹˜ê°€ ì²˜ë¦¬ë˜ëŠ” ê³³ì…ë‹ˆë‹¤. **`validation_loss`**,Â **`validation_accuracy`**Â ì´ëŸ° ë©”íŠ¸ë¦­ì´ ì—¬ê¸°ì„œ ê³„ì‚° ë©ë‹ˆë‹¤.

ë‹¤ë¥¸ ë°©ë²•ë„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- **`training_epoch_end`**: ëª¨ë“  í›ˆë ¨ ì—í¬í¬ê°€ ëë‚  ë•Œ í˜¸ì¶œë©ë‹ˆë‹¤. `training_step`ì´ ë°˜í™˜í•˜ëŠ” ëª¨ë“  ë°ì´í„°ëŠ” ì—¬ê¸°ì—ì„œ ì§‘ê³„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **`validation_epoch_end`**: ëª¨ë“  í›ˆë ¨ ì—í¬í¬ê°€ ëë‚  ë•Œ í˜¸ì¶œë©ë‹ˆë‹¤. `training_step`ì´ ë°˜í™˜í•˜ëŠ” ëª¨ë“  ë°ì´í„°ëŠ” ì—¬ê¸°ì—ì„œ ì§‘ê³„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **`test_step`**: ì´ê²ƒì€ trainerê°€ í…ŒìŠ¤íŠ¸ ë©”ì†Œë“œ ì¦‰, `trainer.test()`ë¡œ í˜¸ì¶œë  ë•Œ í˜¸ì¶œë©ë‹ˆë‹¤.
- **`test_epoch_end`**: ëª¨ë“  í…ŒìŠ¤íŠ¸ ë°°ì¹˜ê°€ ëë‚  ë•Œ í˜¸ì¶œë©ë‹ˆë‹¤.

ë¡œê¹…ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ëª‡ ê°€ì§€ êµ¬ì„±

- SettingÂ **`prog_bar=True` ì§„í–‰ë¥  í‘œì‹œì¤„ì— ë©”íŠ¸ë¦­ì„ í‘œì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.**
- SettingÂ **`on_epoch=True`**, ë©”íŠ¸ë¦­ì€ í•œ ì—í¬í¬ì˜ ì¼ê´„ ì²˜ë¦¬ì—ì„œ ì§‘ê³„ë˜ê³  í‰ê·  ì²˜ë¦¬ë©ë‹ˆë‹¤
- SettingÂ **`on_step=True`**, ê° ë°°ì¹˜ì— ëŒ€í•´ ê¸°ë¡ë©ë‹ˆë‹¤. (useful for loss)

By default:

- Logging inÂ **`training_step`**Â hasÂ **`on_step=True`**
- Logging inÂ **`validation_step`**Â hasÂ **`on_step=False`**,Â **`on_epoch=True`**

ë” ìì„¸í•œ ë‚´ìš©ì€ [!ì—¬ê¸°!](https://pytorch-lightning.readthedocs.io/en/latest/extensions/logging.html#automatic-logging)

ë‹¤ì‹œ ëŒì•„ì™€ì„œ ì–´ë–»ê²Œ ë©”íŠ¸ë¦­ì´ ê³„ì‚°ë˜ê³  ë¡œê·¸ë¥¼ ë‚¨ê¸°ëŠ”ì§€ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.

```python
def training_step(self, batch, batch_idx):
    outputs = self.forward(
        batch["input_ids"], batch["attention_mask"], labels=batch["label"]
    )
    # loss = F.cross_entropy(logits, batch["label"])
    preds = torch.argmax(outputs.logits, 1)
    train_acc = self.train_accuracy_metric(preds, batch["label"])
    self.log("train/loss", outputs.loss, prog_bar=True, on_epoch=True)
    self.log("train/acc", train_acc, prog_bar=True, on_epoch=True)
    return outputs.loss
```

on_epoch=Trueê°€ í™œì„±í™”ë˜ì–´ ìˆìœ¼ë¯€ë¡œ W&B ğŸ‹ï¸ì˜ í”Œë¡¯ì—ëŠ” train/loss_step, train/loss_epoch ë° train/acc_step, train/acc_epochê°€ ìˆìŠµë‹ˆë‹¤.

![image](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/56170880-dcb2-4414-9c6a-252b5cd6f1d7/Untitled.png)

![image](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/821c85b1-e78e-4508-b01a-0b7ced831ba6/Untitled.png)

ê²€ì¦í•˜ëŠ” ë™ì•ˆ Precision, Recall, F1ê³¼ ê°™ì€ ë” ë§ì€ ë©”íŠ¸ë¦­ì„ ëª¨ë‹ˆí„°ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
'''
__init__(self, model ..., lr=3e-5):
...
...
...
self.train_accuracy_metric = torchmetrics.Accuracy()
self.val_accuracy_metric = torchmetrics.Accuracy()
self.f1_metric = torchmetrics.F1(num_classes=self.num_classes)
self.precision_macro_metric = torchmetrics.Precision(
            average="macro", num_classes=self.num_classes
        )
self.recall_macro_metric = torchmetrics.Recall(
            average="macro", num_classes=self.num_classes
        )
self.precision_micro_metric = torchmetrics.Precision(average="micro")
self.recall_micro_metric = torchmetrics.Recall(average="micro")
'''

def validation_step(self, batch, batch_idx):
    labels = batch["label"]
    outputs = self.forward(
        batch["input_ids"], batch["attention_mask"], labels=batch["label"]
    )
    preds = torch.argmax(outputs.logits, 1)

    # Metrics
    valid_acc = self.val_accuracy_metric(preds, labels)
    precision_macro = self.precision_macro_metric(preds, labels)
    recall_macro = self.recall_macro_metric(preds, labels)
    precision_micro = self.precision_micro_metric(preds, labels)
    recall_micro = self.recall_micro_metric(preds, labels)
    f1 = self.f1_metric(preds, labels)

    # Logging metrics
    self.log("valid/loss", outputs.loss, prog_bar=True, on_step=True)
    self.log("valid/acc", valid_acc, prog_bar=True)
    self.log("valid/precision_macro", precision_macro, prog_bar=True)
    self.log("valid/recall_macro", recall_macro, prog_bar=True)
    self.log("valid/precision_micro", precision_micro, prog_bar=True)
    self.log("valid/recall_micro", recall_micro, prog_bar=True)
    self.log("valid/f1", f1, prog_bar=True)
    return {"labels": labels, "logits": outputs.logits}
```

![image](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6c62f5d0-bac8-40d7-ba5f-37afed4eb338/Untitled.png)

`validation_step` ë™ì•ˆ ë°˜í™˜ëœ ê°’ì€ `validation_epoch_end`ì—ì„œ ì§‘ê³„ë  ìˆ˜ ìˆìœ¼ë©° ì´ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ë³€í™˜ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´ ìœ„ì˜ ì½”ë“œ ì¡°ê° ë ˆì´ë¸”ì— í‘œì‹œëœ ê²ƒì²˜ëŸ¼ logitì´ ë°˜í™˜ë©ë‹ˆë‹¤.
ì´ ê°’ì€ `validation_epoch_end` ë©”ì„œë“œì—ì„œ ì§‘ê³„ë  ìˆ˜ ìˆìœ¼ë©° í˜¼ë™ í–‰ë ¬ê³¼ ê°™ì€ ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
def validation_epoch_end(self, outputs):
    labels = torch.cat([x["labels"] for x in outputs]) # ì •ë‹µ
    logits = torch.cat([x["logits"] for x in outputs]) # ì˜ˆì¸¡í•œ logit
    preds = torch.argmax(logits, 1) # argmaxë¥¼ í†µí•´ ì œì¼ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ ê°’ì„ ë°˜í™˜

    cm = confusion_matrix(labels.numpy(), preds.numpy())
```

# ğŸ“ˆ**Adding Plots to**

Logging Metricsì€ ì¶©ë¶„í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë˜í”„ ë° í”Œë¡¯ê³¼ ê°™ì€ ì‹œê°ì  ì •ë³´ê°€ ë§ì„ìˆ˜ë¡ ëª¨ë¸ ì„±ëŠ¥ì„ ë” ì˜ ì´í•´í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.
ê·¸ë˜í”„ë¥¼ ê·¸ë¦¬ëŠ” ë°©ë²•ì€ ì—¬ëŸ¬ ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤. ëª‡ ê°€ì§€ ë°©ë²•ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

**[Document](https://docs.wandb.ai/guides/track/log#custom-charts)**
ì˜ˆë¥¼ ë“¤ì–´ ìœ„ì—ì„œ ê³„ì‚°í•œ Confusion_matrixë¥¼ ê·¸ë¦¬ëŠ” ë°©ë²•ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

## Metchod 1

```python
# 1. Confusion matrix plotting using inbuilt W&B method
self.logger.experiment.log(
    {
# wandb.plot.scatter&bar&line_series&line&histogram&roc_curve ë“±ë“±
        "conf": wandb.plot.confusion_matrix(
            probs=logits.numpy(), y_true=labels.numpy()
        )
    }
)
```

![image](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/7afff83e-c301-4792-8828-dae0d2f66927/Untitled.png)

## Metchod 2

ë˜í•œ, ë¬¸ì„œë¥¼ ì½ë˜ ì¤‘ì— ê¸°ì¡´ì— ì €í¬ê°€ ì‚¬ìš©í•˜ë˜ `matplotlib`ë‚˜ `plotly`ì™€ë„ ê°™ì´ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê²ƒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

```python
import matplotlib.pyplot as plt

plt.plot([1, 2, 3, 4])
plt.ylabel("some interesting numbers")
wandb.log({"chart": plt})
```

ë‹¨ìˆœí•˜ê²Œ matplotlibì˜ figure objectë¥¼ wandb.logì— ë„£ì–´ ì£¼ì‹œë©´ ë©ë‹ˆë‹¤.
ë§Œì•½ `â€œYou attempted to log an empty plotâ€` ê°™ì€ ì—ëŸ¬ë¥¼ ë§Œë‚˜ì…¨ë‹¤ë©´ `fig= plt.figure()` ì— ë„£ì–´ ì£¼ì‹  ë‹¤ìŒì— ì§„í–‰í•˜ì‹œë©´ ë©ë‹ˆë‹¤.

ë˜ëŠ”

```python
# 3. Confusion Matric plotting using Seaborn
data = confusion_matrix(labels.numpy(), preds.numpy())
df_cm = pd.DataFrame(data, columns=np.unique(labels), index=np.unique(labels))
df_cm.index.name = "Actual"
df_cm.columns.name = "Predicted"
plt.figure(figsize=(10, 5))
plot = sns.heatmap(
    df_cm, cmap="Blues", annot=True, annot_kws={"size": 16}
)  # font size
self.logger.experiment.log({"Confusion Matrix": wandb.Image(plot)})
```

![image](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/bbd9820f-5be8-4e47-b313-5ad5cd505e2d/Untitled.png)

## Metchod 3

`matplotlib` ë¿ë§Œ ì•„ë‹ˆë¼ `scikit-learn`ë„ ì§€ì›í•´ì¤€ë‹¤ê³  í•©ë‹ˆë‹¤.

```python
# 2. Confusion Matrix plotting using scikit-learn method
wandb.log({"cm": wandb.sklearn.plot_confusion_matrix(labels.numpy(), preds)})
```

![image](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/82d05491-0be5-4910-a81a-c9a3d9e54c92/Untitled.png)

# ğŸ“‘**Adding Data samples to**

ëª¨ë¸ì´ í•™ìŠµë˜ë©´ ëª¨ë¸ì´ ì˜ ìˆ˜í–‰ë˜ëŠ” ë¶€ë¶„ê³¼ ê·¸ë ‡ì§€ ì•Šì€ ë¶€ë¶„ì„ ì´í•´í•´ì•¼ í•©ë‹ˆë‹¤.
ìš°ë¦¬ëŠ” `cola` ë¬¸ì œì— ëŒ€í•´ ì‘ì—… ì¤‘ì´ë¯€ë¡œ ëª¨ë¸ì´ ì˜ ìˆ˜í–‰ë˜ì§€ ì•ŠëŠ” ëª‡ ê°€ì§€ ìƒ˜í”Œì„ ë³´ê³  W&Bì— ê¸°ë¡í•´ ë³´ê² ìŠµë‹ˆë‹¤.

ë°ì´í„°ë¥¼ í”Œë¡œíŒ…í•˜ëŠ” ë°©ë²•ì—ëŠ” ì—¬ëŸ¬ ê°€ì§€ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ìì„¸í•œ ë‚´ìš©ì€ ì—¬ê¸°ì—ì„œ [ë¬¸ì„œ](https://docs.wandb.ai/guides/data-vis/tables-quickstart#1-log-a-table)ë¥¼ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤.

ì´ê²ƒì€ âš¡ï¸ì—ì„œ ì½œë°± ğŸ” ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
class SamplesVisualisationLogger(pl.Callback):
    def __init__(self, datamodule):
        super().__init__()

        self.datamodule = datamodule

    def on_validation_end(self, trainer, pl_module):
        # can be done on complete dataset also
        val_batch = next(iter(self.datamodule.val_dataloader()))
        sentences = val_batch["sentence"]

        # get the predictions
        outputs = pl_module(val_batch["input_ids"], val_batch["attention_mask"])
        preds = torch.argmax(outputs.logits, 1)
        labels = val_batch["label"]

        # predicted and labelled data
        df = pd.DataFrame(
            {"Sentence": sentences, "Label": labels.numpy(), "Predicted": preds.numpy()}
        )

        # wrongly predicted data
        wrong_df = df[df["Label"] != df["Predicted"]]

        # Logging wrongly predicted dataframe as a table
        trainer.logger.experiment.log(
            {
                "examples": wandb.Table(dataframe=wrong_df, allow_mixed_types=True),
                "global_step": trainer.global_step,
            }
        )
```

ê·¸ë¦¬ê³  ë‚˜ì„œ ì½œë°±ğŸ”ì„ ë„£ì–´ ì¤ë‹ˆë‹¤!

```python
trainer = pl.Trainer(
        max_epochs=3,
        logger=wandb_logger,
        callbacks=[checkpoint_callback, SamplesVisualisationLogger(cola_data)],
        log_every_n_steps=10,
        deterministic=True,
    )
```

![image](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/baff1c86-e375-4ee3-a8c0-12761ef8920e/Untitled.png)

# **ğŸ”š**
